#!crnn/rnn.py
# kate: syntax python;
# -*- mode: python -*-
# sublime: syntax 'Packages/Python Improved/PythonImproved.tmLanguage'
# vim:set expandtab tabstop=4 fenc=utf-8 ff=unix ft=python:

import os
import numpy

use_tensorflow = True

# data
extern_data = {
    "data": {
        "dim": 40  # MFCC dim
    },
    "classes": {
        "dim": 10025,  # BPE vocab size
        "sparse": True
    }
}
_EpochSplit = 20  # split up the training epoch into parts
_Parts = [
    "train-clean-100", "train-clean-360", "train-other-500",
    "dev-clean", "dev-other",
    "test-clean", "test-other"]


def get_dataset(key, subset=None, train_partition_epoch=None):
    files = []
    parts = [part for part in _Parts if part.startswith(key)]
    assert parts
    for part in parts:
        files += ["data/dataset-ogg/%s.zip" % part, "data/dataset-ogg/%s.txt.gz" % part]
    d = {
        "class": 'OggZipDataset',
        "path": files,
        "zip_audio_files_have_name_as_prefix": False,
        "targets": {
            'bpe_file': 'data/dataset/trans.bpe.codes',
            'vocab_file': 'data/dataset/trans.bpe.vocab',
            'seq_postfix': [0],  # for label-sync models (e.g. attention-based enc-dec), need EOS
            'unknown_label': None  # should not be needed (but we allow below for non-train)
            },
        "audio": {
            "norm_mean": "data/dataset/stats.mean.txt",
            "norm_std_dev": "data/dataset/stats.std_dev.txt"},
    }
    if key.startswith("train"):
        d["partition_epoch"] = train_partition_epoch
        if key == "train":
            d["epoch_wise_filter"] = {
                (1, 5): {
                    'max_mean_len': 75,  # chars, should be around 14 bpe labels
                    'subdirs': ['train-clean-100', 'train-clean-360']}}
        #d["audio"]["random_permute"] = True  # play around. note that this can be slow
        d["seq_ordering"] = "laplace:.1000"
    else:
        d["targets"]['unknown_label'] = '<unk>'  # only for non-train. for train, there never should be an unknown
        d["fixed_random_seed"] = 1
        d["seq_ordering"] = "sorted_reverse"
    if subset:
        d["fixed_random_subset"] = subset  # faster
    return d


train = get_dataset("train", train_partition_epoch=_EpochSplit)
dev = get_dataset("dev", subset=3000)


# log
# log = "data/exp-%s/returnn.%s.$date.log" % (setup_name, task)
log = None
log_verbosity = 5  # change to 4 or 3 if it is too verbose for you

