#!rnn.py

# external parameters set explicitely by sisyphus
ext_model = config.value("ext_model", None)
ext_learning_rate_file = config.value("ext_learning_rate_file", None)

# external parameters set by parameter dictionary
# training
ext_num_epochs = config.int("ext_num_epochs", 0)

# data
ext_partition_epoch = config.int("ext_partition_epoch", 1)
ext_training_zips = eval(config.value("ext_training_zips", "None"))
ext_dev_zips = eval(config.value("ext_dev_zips", "None"))

ext_norm_mean_value = float(config.value("ext_norm_mean_value", 0))
ext_norm_std_dev_value = float(config.value("ext_norm_std_dev_value", 0))

ext_forward = config.value("ext_forward", False)
ext_eval_features = config.value("ext_eval_features", None)
ext_eval_segment_file = config.value("ext_segment_file", None)
ext_load_epoch = int(config.value("ext_load_epoch", 0))

def get_dataset(key):
  dataset = {
    'class': 'MetaDataset',
    'data_dims': {'data': (80, 2), 'data_target': (512, 2)},
    'data_map': {'data': ('dataset_source', 'data'), 'data_target': ('dataset_target', 'data')},
    'datasets': { 'dataset_source': { 'audio': { 'feature_options': {'fmin': 60},
                                                 'features': 'db_mel_filterbank',
                                                 'norm_mean': ext_norm_mean_value,
                                                 'norm_std_dev': ext_norm_std_dev_value,
                                                 'num_feature_filters': 80,
                                                 'peak_normalization': False,
                                                 'preemphasis': 0.97,
                                                 'step_len': 0.0125,
                                                 'window_len': 0.05},
                                      'class': 'OggZipDataset',
                                      'partition_epoch': ext_partition_epoch,
                                      'targets': None},
                  'dataset_target': { 'audio': { 'features': 'linear_spectrogram',
                                                 'num_feature_filters': 512,
                                                 'peak_normalization': False,
                                                 'preemphasis': 0.97,
                                                 'step_len': 0.0125,
                                                 'window_len': 0.05},
                                      'class': 'OggZipDataset',
                                      'partition_epoch': ext_partition_epoch,
                                      'targets': None}},
    'partition_epoch': ext_partition_epoch,
    'seq_ordering': 'laplace:12'}

  if key == "dev":
    dataset['datasets']['dataset_source']['path'] = ext_dev_zips
    dataset['datasets']['dataset_target']['path'] = ext_dev_zips
  elif key == "train":
    dataset['datasets']['dataset_source']['path'] = ext_training_zips
    dataset['datasets']['dataset_target']['path'] = ext_training_zips
  elif key == "eval":
    return {'class': 'HDFDataset', 'files': [ext_eval_features], "cache_byte_size": 0}
  else:
    assert False, "invalid dataset key"

  return dataset

adam = True
batch_size = 18000
batching = 'random'
cleanup_old_models = True
debug_add_check_numerics_on_output = False
debug_mode = False
dev = get_dataset('dev')
device = 'gpu'
extern_data = {'data': (80, 2), 'data_target': (512, 2)}
gradient_clip = 1
gradient_noise = 0.0
learning_rate = 0.001
learning_rate_control = 'newbob_multi_epoch'
learning_rate_control_min_num_epochs_per_new_lr = 5
learning_rate_control_relative_error_relative_lr = True
learning_rate_file = ext_learning_rate_file
learning_rates = [0.001]
log = ['./crnn.log']
log_batch_size = True
log_verbosity = 5
max_seq_length = {'data': 1200}
max_seqs = 200
model = ext_model
multiprocessing = True
network = { 'add': {'class': 'combine', 'from': ['combine_0', 'combine_1'], 'kind': 'add'},
  'combine_0': {'class': 'copy', 'from': ['lstm0_fw', 'lstm0_bw']},
  'combine_1': {'class': 'copy', 'from': ['lstm1_fw', 'lstm1_bw']},
  'lstm0_bw': {'class': 'rec', 'direction': -1, 'from': ['data'], 'n_out': 512, 'unit': 'nativelstm2'},
  'lstm0_fw': {'class': 'rec', 'direction': 1, 'from': ['data'], 'n_out': 512, 'unit': 'nativelstm2'},
  'lstm1_bw': {'class': 'rec', 'direction': -1, 'dropout': 0.2, 'from': ['lstm0_fw', 'lstm0_bw'], 'n_out': 512, 'unit': 'nativelstm2'},
  'lstm1_fw': {'class': 'rec', 'direction': 1, 'dropout': 0.2, 'from': ['lstm0_fw', 'lstm0_bw'], 'n_out': 512, 'unit': 'nativelstm2'},
  'output': {'activation': 'relu', 'class': 'linear', 'from': ['add', 'transform'], 'loss': 'mse', 'n_out': 512, 'target': 'data_target'},
  'transform': {'activation': 'relu', 'class': 'linear', 'from': ['data'], 'n_out': 512}}
newbob_learning_rate_decay = 0.9
newbob_multi_num_epochs = 3
newbob_multi_update_interval = 1
newbob_relative_error_threshold = 0
num_epochs = ext_num_epochs
optimizer_epsilon = 1e-08
save_interval = 1
stop_on_nonfinite_train_score = False
target = 'classes'
task = 'train'
tf_log_memory_usage = True
train = get_dataset('train')
truncation = -1
update_on_device = True
use_learning_rate_control_always = True
use_tensorflow = True


if ext_forward:
  task = "forward"
  load_epoch = ext_load_epoch
  train = None
  dev = None
  eval = get_dataset('eval')
  max_seq_length = 0
  max_seqs=50
  output_file = "linear_features.hdf"