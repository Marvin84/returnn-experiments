#!crnn/rnn.py
# kate: syntax python;
# see also file:///u/zeyer/setups/quaero-en/training/quaero-train11/50h/ann/2015-07-29--lstm-gt50/config-train/dropout01.3l.n500.custom_lstm.adam.lr1e_3.config

import os
import numpy
from subprocess import check_output

# task
use_tensorflow = True
task = "train"
device = "gpu"
multiprocessing = True
update_on_device = True

_cf_cache = {}

def cf(filename):
    """Cache manager"""
    if filename in _cf_cache:
        return _cf_cache[filename]
    if check_output(["hostname"]).strip().decode("utf8") in ["cluster-cn-217", "sulfid", "zink", "cobalt", "niob"]:
        print("use local file: %s" % filename)
        return filename  # for debugging
    cached_fn = check_output(["cf", filename]).strip().decode("utf8")
    assert os.path.exists(cached_fn)
    _cf_cache[filename] = cached_fn
    return cached_fn

# data
context_window = 1

window = 1 # 30-1-30
feature_dim = 40  # Gammatone 40-dim
channel_num = 1
num_inputs = feature_dim * channel_num * window
num_outputs = 9001  # CART labels
EpochSplit = 6

def get_sprint_dataset(data):
    assert data in ["train", "cv"]
    epochSplit = {"train": EpochSplit, "cv": 1}

    # see /u/tuske/work/ASR/switchboard/corpus/readme
    # and zoltans mail https://mail.google.com/mail/u/0/#inbox/152891802cbb2b40
    files = {}
    files["config"] = "config/training.config"
    #files["corpus"] = "/u/tuske/work/ASR/switchboard/corpus/xml/train.corpus.gz"    
    files["corpus"] = "/u/corpora/speech/switchboard-1/xml/swb1-all/swb1-all.corpus.gz"
    files["segments"] = "dependencies/seg_%s" % {"train":"train", "cv":"cv_head3000"}[data]
    files["features"] = "/u/tuske/work/ASR/switchboard/feature.extraction/gt40_40/data/gt.train.bundle"
    #files["features"] = "/work/asr2/bozheniuk/feature_extraction/logmel/logmel.train_v2.cache.1"
    #files["features"] = "/u/bozheniuk/setups/switchboard/feature_extraction/cluster_setup/logmel64_30/data/logmel.train.bundle"    
    files["lexicon"] = "/u/tuske/work/ASR/switchboard/corpus/train.lex.v1_0_3.ci.gz"
    files["alignment"] = "dependencies/tuske__2016_01_28__align.combined.train"
    files["cart"] = "/u/tuske/work/ASR/switchboard/initalign/data/%s" % {9001: "cart-9000"}[num_outputs]
    for k, v in sorted(files.items()):
        assert os.path.exists(v), "%s %r does not exist" % (k, v)
    estimated_num_seqs = {"train": 227047, "cv": 3000}  # wc -l segment-file

    # features: /u/tuske/work/ASR/switchboard/feature.extraction/gt40_40/data/gt.train.*
    args = [
    "--config=" + files["config"],
    lambda: "--*.corpus.file=" + cf(files["corpus"]),
    lambda: "--*.corpus.segments.file=" + cf(files["segments"]),
    {"train": "--*.corpus.segment-order-shuffle=true", "cv": "--*.corpus.segment-order-sort-by-time-length=true"}[data],
    "--*.state-tying.type=cart",
    lambda: "--*.state-tying.file=" + cf(files["cart"]),
    "--*.trainer-output-dimension=%i" % num_outputs,
    lambda: "--*.lexicon.file=" + cf(files["lexicon"]),
    lambda: "--*.alignment-cache-path=" + cf(files["alignment"]),
    lambda: "--*.feature-cache-path=" + cf(files["features"]),
    #"--*.mean-normalization.file=dependencies/setup-base/step254-hybrid-mlp-ibm-cmllr/mlp.4/train_mlp/normalize_layer1/mean",
    #"--*.variance-normalization.file=dependencies/setup-base/step254-hybrid-mlp-ibm-cmllr/mlp.4/train_mlp/normalize_layer1/std",
    "--*.log-channel.file=log/crnn.sprint.train-dataset.xml",
    "--*.window-size=1",
    "--*.trainer-output-dimension=%i" % num_outputs
    ]
    return {
    "class": "ExternSprintDataset", "sprintTrainerExecPath": "sprint-executables/nn-trainer",
    "sprintConfigStr": args,
    "input_stddev": 3.,
    "partitionEpoch": epochSplit[data],
    "estimated_num_seqs": estimated_num_seqs[data] // (epochSplit[data] or 1),
#    "context_window": context_window
#    "window": window
    }
    #"window": window
cache_size = "0"
sprint_interface_dataset_opts = {"input_stddev": 3.}
# network
# (also defined by num_inputs & num_outputs)
dropout = 0
ldropout = 0.05
L2 = 0.1
# filter_size = (3, 3)  # for 2D conv on (window, feature) axes

# bn params
masked_time = False
fused = True
axes = ["f"]

bn_momentum = 0.997
bn_epsilon = 1e-5

cur_feat_dim = feature_dim
network = {}
_last = "data"
def add_sequential_layer(name, d, from_=None):
    global _last, network
    assert "from" not in d
    if from_ is not None:
        d["from"] = from_
    else:
        d["from"] = [_last]
    assert name not in network
    network[name] = d
    _last = name       


def jump_net(prefix, inputs, filters, dilation_rate, data_format, conv_time_dim=False):
    if data_format == 'channels_first':         
        NCHW = True
    else:
        NCHW = False
    filter_size = (3, 3)
    strides = (1, 1)
    #dilation_rate = (1,)
    padding = "SAME"
    use_frequency = False
    add_sequential_layer("%s_c1" % prefix, {"class": "conv", "n_out": filters, "filter_size": filter_size, "auto_use_channel_first": NCHW,
                         "strides": strides, "dilation_rate": (dilation_rate, 1), "padding": padding, "activation": None, "with_bias": False, "dropout": dropout, 
                         "forward_weights_init": "xavier", "L2": L2})
    add_sequential_layer("%s_bn1" % prefix, {"class": "batch_norm", "masked_time": masked_time, "axes": axes, "fused": fused, "momentum": bn_momentum, "epsilon": bn_epsilon})
    add_sequential_layer("%s_y1" % prefix, {"class": "activation", "activation": "relu", "batch_norm": False})
    add_sequential_layer("%s_c2" % prefix, {"class": "conv", "n_out": filters, "filter_size": filter_size, "auto_use_channel_first": NCHW,
                         "strides": strides, "dilation_rate": (dilation_rate, 1), "padding": padding, "activation": None, "with_bias": False, "dropout": dropout, 
                         "forward_weights_init": "xavier", "L2": L2})

    add_sequential_layer("%s_p" % prefix, {"class": "combine", "kind": "add"}, from_=["%s_c2" % prefix, inputs])
    add_sequential_layer("%s_bn2" % prefix, {"class": "batch_norm", "masked_time": masked_time, "axes": axes, "fused": fused, "momentum": bn_momentum, "epsilon": bn_epsilon})
    add_sequential_layer("%s_o" % prefix, {"class": "activation", "activation": "relu", "batch_norm": False})
    return "%s_o" % prefix


def self_attention(prefix, inputs):
    context_size = 31

    crop_left = (context_size - 1) // 2
    crop_right = (context_size - 1) - crop_left

    querry_dim = 40
    querry_dim += context_size
    key_dim = 40
    value_dim = 80

    key_scale = 1.0 / (float(key_dim) ** .5)
    
    num_heads = 15
  
    # Q, K, V
    add_sequential_layer("%s_att_q" % prefix, {"class": "linear", "activation": None, "with_bias": False, "n_out": querry_dim * num_heads}, from_=inputs)
    add_sequential_layer("%s_att_k" % prefix, {"class": "linear", "activation": None, "with_bias": False, "n_out": key_dim * num_heads}, from_=inputs)
    add_sequential_layer("%s_att_v" % prefix, {"class": "linear", "activation": None, "with_bias": False, "n_out": value_dim * num_heads}, from_=inputs)

    # split heads
    add_sequential_layer("%s_att_q_split" % prefix, {"class": "split_dims", "axis": "f", "dims": (num_heads, querry_dim)}, from_="%s_att_q" % prefix)
    add_sequential_layer("%s_att_k_split" % prefix, {"class": "split_dims", "axis": "f", "dims": (num_heads, key_dim)}, from_="%s_att_k" % prefix)
    add_sequential_layer("%s_att_v_split" % prefix, {"class": "split_dims", "axis": "f", "dims": (num_heads, value_dim)}, from_="%s_att_v" % prefix)

    # crop
    add_sequential_layer("%s_att_queries_crop" % prefix, {"class": "slice", "axis": "t", "slice_start": crop_left, "slice_end": -crop_right}, from_="%s_att_q_split" % prefix)
    
    # split queries into key and context parts
    add_sequential_layer("%s_att_queries_key_part" % prefix, {"class": "slice", "axis": "f", "slice_start": 0, "slice_end": querry_dim - context_size}, from_="%s_att_queries_crop" % prefix)
    add_sequential_layer("%s_att_queries_context_part" % prefix, {"class": "slice", "axis": "f", "slice_start": querry_dim - context_size, "slice_end": querry_dim}, from_="%s_att_queries_crop" % prefix)

    add_sequential_layer("%s_att_key_window" % prefix, {"class": "window", "window_size": context_size, "padding": "valid"}, from_="%s_att_k_split" % prefix)
    add_sequential_layer("%s_att_value_window" % prefix, {"class": "window", "window_size": context_size, "padding": "valid"}, from_="%s_att_v_split" % prefix)
    
    # energy
    add_sequential_layer("%s_att_energy_dot" % prefix, {"class": "dot", "red1": "f", "red2": "f", "var1": None, "var2": "s:1"}, from_=["%s_att_queries_key_part" % prefix, "%s_att_key_window" % prefix])
    add_sequential_layer("%s_att_energy_scaled" % prefix, {"class": "eval", "eval": "source(0) * %f" % key_scale}, from_="%s_att_energy_dot" % prefix)
    add_sequential_layer("%s_att_energy_biased" % prefix, {"class": "combine", "kind": "add"}, from_=["%s_att_energy_scaled" % prefix, "%s_att_queries_context_part" % prefix])

    # weights
    add_sequential_layer("%s_att_weights_softmax" % prefix, {"class": "activation", "activation": "softmax"}, from_="%s_att_energy_biased" % prefix)  
    
    # output
    add_sequential_layer("%s_att_out_dot" % prefix, {"class": "dot", "red1": "f", "red2": "s:1", "var1": None, "var2": "f"}, from_=["%s_att_weights_softmax" % prefix, "%s_att_value_window" % prefix])
    add_sequential_layer("%s_att_out_combine" % prefix, {"class": "merge_dims", "axes": ["s:1", "f"]}, from_="%s_att_out_dot" % prefix)

    # activation
    add_sequential_layer("%s_att_activation" % prefix, {"class": "activation", "activation": "relu"})

    # batch norm
    add_sequential_layer("%s_att_bn" % prefix, {"class": "batch_norm", "masked_time": masked_time, "axes": axes, "fused": False, "momentum": bn_momentum, "epsilon": bn_epsilon})
    return "%s_att_bn" % prefix
   

def jump_block(prefix, inputs, jump_net_num, filters, data_format, dilation_rate, attention=False, conv_time_dim=False):
    if data_format == 'channels_first':         
        NCHW = True
    else:
        NCHW = False    
    filter_size = (3, 3)
    strides = (1, 1)
    padding = "SAME"
    use_frequency = False
    add_sequential_layer("%s_c" % prefix, {"class": "conv", "n_out": filters, "filter_size": filter_size, "auto_use_channel_first": NCHW,
                         "strides": strides, "dilation_rate": dilation_rate, "padding": padding, "activation": None, "with_bias": False, "dropout": dropout, 
                         "forward_weights_init": "xavier", "L2": L2})
    # doing strides
    add_sequential_layer("%s_c_strides" % prefix, {"class": "slice", "axis": "s:1", "slice_step": 2})

    dilation_rate *= 2
    # print("dilation_rate: ", dilation_rate)
    inputs = "%s_c_strides" % prefix
    for i in range(1, jump_net_num + 1):
        inputs = jump_net("%s_net%i" % (prefix, i + 1), inputs, filters, dilation_rate, data_format=data_format, conv_time_dim=conv_time_dim)
    #add_sequential_layer("%s_mat_prod" % prefix, {"class": "elemwise_prod", "axes": ("s:0", "s:1")})       
    
    if attention:
        inputs = self_attention(prefix, inputs)
    return inputs


def lacea(inputs):
    block_sizes = [2, 2, 2, 2]
    filters = 128
    data_format = 'channels_last'
    conv_time_dim = True
    dilation_rate = 1
    attention = False

    for i, num_blocks in enumerate(block_sizes):    
        inputs = jump_block("block%i" % (i + 1), inputs, num_blocks, filters, data_format, dilation_rate, attention=attention, conv_time_dim=conv_time_dim)
        filters *= 2
        dilation_rate *= 2
    
    # add_sequential_layer("output_merge_dims", {"class": "merge_dims", "axes": ["s:1", "f"]}, from_=inputs)
    # swap to channels last before attention
    # add_sequential_layer("output_swap_axes", {"class": "swap_axes", "axis1": "s:0", "axis2": "f"}, from_="output_merge_dims")
    

    # inputs = self_attention("output", inputs="output_merge_dims")
    
    return

#paramnum 22074089

add_sequential_layer("split", {"class": "split_dims", "axis": "f", "dims": (feature_dim, channel_num)}) # output: (batch, time, window = 61, feature = 40, channel = 1)
#add_sequential_layer("merge_in", {"class": "merge_dims", "axes": "bt"}) #output: (batch * time, window = 61, features = 40, channel = 1) 
lacea(inputs="data")
#add_sequential_layer("split_batch_time", {"class": "split_batch_time"})
add_sequential_layer("out_pool", {"class": "reduce", "mode": "avg", "axes": "s:1", "keep_dims": False})
add_sequential_layer("output", {"class": "softmax", "loss": "ce", "L2": L2, "n_out": num_outputs, "loss_opts": {"focal_loss_factor": 2.0}, "dropout": ldropout})

#print(network)
#context_window = int(2 * (30 // 2) + 1)
train = get_sprint_dataset("train")
dev = get_sprint_dataset("cv")

############## debug stuff

debug_print_layer_output_template = True  # useful for debugging
#debug_print_layer_output_sizes = True
#debug_print_layer_output_shape = True  # might be useful for debugging
#debug_shell_in_runner = True
log_batch_size = True
tf_log_memory_usage = True

############## debug stuff

# trainer
batching = "random"
batch_size = 32 * (150)
max_seqs = 500
chunking = "150:150"
truncation = -1
num_epochs = 120
#pretrain = "default"
#pretrain_construction_algo = "from_output"


gradient_clip = 0
#nadam = True
gradient_noise = 0.1
momentum = 0.99
learning_rate = 5e-6
#optimizer_epsilon = 1.0
learning_rate_file = "newbob.data"
learning_rate_control = "newbob_multi_epoch"
learning_rate_control_relative_error_relative_lr = True
newbob_multi_num_epochs = 6
newbob_multi_update_interval = 1
model = "net-model/network"

cleanup_old_models = True
store_metadata_mod_step = None

# log
log = "log/crnn.train.log"
log_verbosity = 5
