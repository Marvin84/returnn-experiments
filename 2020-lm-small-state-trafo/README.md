Small state Transformer language model training example configs, from the paper:

How Much Self-Attention Do We Need? Trading Attention for Feed-Forward Layers.
```
@InProceedings {irie:icassp20,
author= {Irie, Kazuki and Gerstenberger, Alexander and Schl√ºter, Ralf and Ney, Hermann},
title= {How Much Self-Attention Do We Need? Trading Attention for Feed-Forward Layers},
booktitle= {IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
year= 2020,
address= {Barcelona, Spain},
month= may,
}
```
