#!rnn.py
# kate: syntax python;	
# -*- mode: python -*-	
# sublime: syntax 'Packages/Python Improved/PythonImproved.tmLanguage'	
# vim:set expandtab tabstop=4 fenc=utf-8 ff=unix ft=python:

import os
import numpy
from subprocess import check_output, CalledProcessError
from Pretrain import WrapEpochValue

import sys 
sys.path.append('/u/bahar/workspace/st/iwslt2020/st--en-es-20200720/config-train/extra_module/')
from add import AddOneHotToTime
from TFNetworkLayer import auto_register_layer_classes
auto_register_layer_classes('/u/bahar/workspace/st/iwslt2020/st--en-es-20200720/config-train/extra_module/add.py')


# task
use_tensorflow = True
task = config.value("task", "train")
device = "gpu"
multiprocessing = True
update_on_device = True

debug_mode = False

if config.has("beam_size"):
    beam_size = config.int("beam_size", 0)
    print("** beam_size %i" % beam_size)
else:
    beam_size = 12

if config.has("beam_size_asr"):
    beam_size_asr = config.int("beam_size_asr", 0)
    print("** beam_size_asr %i" % beam_size_asr)
else:
    beam_size_asr = 12

if config.has("prob_scale"):
    prob_scale = config.float("prob_scale", 1.0)
    print("** prob_scale %4.2f" % prob_scale)
else:
    prob_scale = 1.0

_cf_cache = {}

def cf(filename):
    """Cache manager"""
    if filename in _cf_cache:
        return _cf_cache[filename]
    if debug_mode or check_output(["hostname"]).strip().decode("utf8") in ["cluster-cn-211", "valin"]:
        print("use local file: %s" % filename)
        return filename  # for debugging
    try:
        cached_fn = check_output(["cf", filename]).strip().decode("utf8")
    except CalledProcessError:
        print("Cache manager: Error occured, using local file")
        return filename
    assert os.path.exists(cached_fn)
    _cf_cache[filename] = cached_fn
    return cached_fn


num_inputs = 80  # 
num_outputs = {"target_text": (35209, 1), "data": (num_inputs, 2), "target_text_sprint": (5131, 1)}  # see vocab
target_st = "target_text"
target_asr = "target_text_sprint"

# data
if task == "search":
    num_inputs = 80  # 
    num_outputs = {"orth_classes": (35209, 1), "data": (num_inputs, 2), "orth_classes_sprint": (5131, 1)}  # see vocab
    target_st = "orth_classes"
    target_asr = "orth_classes_sprint"
    data = "data"

EpochSplit = 5

def get_sprint_dataset(data, is_search=False):
    #assert data in {"train", "dev", "tstHE", "tstCOMMON"}
    files = {}
    files["features"] = "/work/bahar/st/iwslt2020/data/final_data/mustc/mfcc.%s.bundle" % data
    files["config"] = "/u/bahar/workspace/asr/librispeech/end2end-en-fr-asr--2018-11-22/config/training.config"
    files["corpus"] = "/work/bahar/st/iwslt2020/data/final_data/mustc/%s_bpe5k_offlist.xml" % data

    if data in ["train", "dev"]:
       files["segments"] = "/work/bahar/st/iwslt2020/data/final_data/mustc/seg_%s" % data
    for k, v in sorted(files.items()):
        assert os.path.exists(v), "%s %r does not exist" % (k, v)
    estimated_num_seqs = {"train": 221957, "dev": 1423}  # wc -l
    args = [
        "--config=" + files["config"],
        lambda: "--*.corpus.file=" + cf(files["corpus"]),
        lambda: "--*.corpus.segments.file=" + (cf(files["segments"]) if "segments" in files else ""),
        "--*.corpus.segment-order-shuffle=false",
        "--*.segment-order-sort-by-time-length=false",
        "--*.segment-order-sort-by-time-length-chunk-size=%i" % {"train": (EpochSplit or 1) * 1000}.get(data, -1),
        lambda: "--*.feature-cache-path=" + cf(files["features"]),
        "--*.log-channel.file=/dev/null",
        #"--*.log-channel.file=sprint-%s.log" % data,
        #"--*.log-channel.unbuffered=true",
        "--*.window-size=1",
    ]
    d = {
        "class": "ExternSprintDataset", "sprintTrainerExecPath": "/work/smt3/bahar/expriments/st/iwslt2018/features/direct-st/shared/sprint-executables/nn-trainer",
        "sprintConfigStr": args,
        "partitionEpoch": 1,
        "estimated_num_seqs": estimated_num_seqs[data] if data in estimated_num_seqs else None,
        "input_stddev": 3.,
        "orth_vocab": {
        "vocab_file": "/work/bahar/st/iwslt2020/data/final_data/mustc/target.vocab.pkl" if is_search else "/work/bahar/st/iwslt2020/data/bpe-en/bpe-offlists/en.vocab.5k.dict",
        "unknown_label": "<UNK>",
        "seq_postfix": [0] }
    }
    return d

train_sprint = get_sprint_dataset("train")
dev_sprint = get_sprint_dataset("dev")
devmustc = get_sprint_dataset("dev", is_search=True)
tstHE = get_sprint_dataset("tstHE", is_search=True)
tstCOMMON = get_sprint_dataset("tstCOMMON", is_search=True)

translationDataset = {"class": "TranslationDataset",
                      "path": "/work/bahar/st/iwslt2020/data/final_data/mustc/",
                      "file_postfix": "train", 
                      "unknown_label": "<UNK>",
                      "source_postfix": " </S>",
                      "target_postfix": " </S>"}
                      #"seq_ordering": "sorted"}

train_translation = translationDataset.copy()

train = {"class": "MetaDataset", "seq_list_file": "/work/bahar/st/iwslt2020/data/final_data/mustc/train_seq.pkl", "seq_lens_file": "",
    "datasets": {"sprint": train_sprint, "translation": train_translation},
    "data_map": {"data": ("sprint", "data"),
                 "target_text_sprint": ("sprint", "orth_classes"),
                 "source_text": ("translation", "data"),
                 "target_text": ("translation", "classes")},
    "data_dims": {"data": [80, 2],
                  "target_text_sprint": [5131, 1],
                  "source_text": [5131, 1],
                  "target_text": [35209, 1]},
    "seq_ordering": "random",
    "partition_epoch": EpochSplit
}

dev_translation = translationDataset.copy()
dev_translation["file_postfix"] = "dev"
dev = train.copy()
dev["seq_list_file"] = "/work/bahar/st/iwslt2020/data/final_data/mustc/dev_seq.pkl"
dev["datasets"] = {"sprint": dev_sprint, "translation": dev_translation}
dev["partition_epoch"] = None
dev["seq_ordering"] = "default"

cache_size = "0"
window = 1

if task == "search":
    dev = get_sprint_dataset("dev", is_search=True)

# network
# (also defined by num_inputs & num_outputs)
EncKeyTotalDim = 1024
AttNumHeads = 1
EncKeyPerHeadDim = EncKeyTotalDim // AttNumHeads
EncValueTotalDim = 2048
EncValuePerHeadDim = EncValueTotalDim // AttNumHeads
LstmDim = EncValueTotalDim // 2


def summary(name, x):
    """
    :param str name:
    :param tf.Tensor x: (batch,time,feature)
    """
    import tensorflow as tf
    # tf.summary.image wants [batch_size, height,  width, channels],
    # we have (batch, time, feature).
    img = tf.expand_dims(x, axis=3)  # (batch,time,feature,1)
    img = tf.transpose(img, [0, 2, 1, 3])  # (batch,feature,time,1)
    tf.summary.image(name, img, max_outputs=10)
    tf.summary.scalar("%s_max_abs" % name, tf.reduce_max(tf.abs(x)))
    mean = tf.reduce_mean(x)
    tf.summary.scalar("%s_mean" % name, mean)
    stddev = tf.sqrt(tf.reduce_mean(tf.square(x - mean)))
    tf.summary.scalar("%s_stddev" % name, stddev)
    tf.summary.histogram("%s_hist" % name, tf.reduce_max(tf.abs(x), axis=2))


def _mask(x, batch_axis, axis, pos, max_amount):
    """
    :param tf.Tensor x: (batch,time,feature)
    :param int batch_axis:
    :param int axis:
    :param tf.Tensor pos: (batch,)
    :param int|tf.Tensor max_amount: inclusive
    """
    import tensorflow as tf
    ndim = x.get_shape().ndims
    n_batch = tf.shape(x)[batch_axis]
    dim = tf.shape(x)[axis]
    amount = tf.random_uniform(shape=(n_batch,), minval=1, maxval=max_amount + 1, dtype=tf.int32)
    pos2 = tf.minimum(pos + amount, dim)
    idxs = tf.expand_dims(tf.range(0, dim), 0)  # (1,dim)
    pos_bc = tf.expand_dims(pos, 1)  # (batch,1)
    pos2_bc = tf.expand_dims(pos2, 1)  # (batch,1)
    cond = tf.logical_and(tf.greater_equal(idxs, pos_bc), tf.less(idxs, pos2_bc))  # (batch,dim)
    if batch_axis > axis:
        cond = tf.transpose(cond)  # (dim,batch)
    cond = tf.reshape(cond, [tf.shape(x)[i] if i in (batch_axis, axis) else 1 for i in range(ndim)])
    from TFUtil import where_bc
    x = where_bc(cond, 0.0, x)
    return x


def random_mask(x, batch_axis, axis, min_num, max_num, max_dims):
    """
    :param tf.Tensor x: (batch,time,feature)
    :param int batch_axis:
    :param int axis:
    :param int|tf.Tensor min_num:
    :param int|tf.Tensor max_num: inclusive
    :param int|tf.Tensor max_dims: inclusive
    """
    import tensorflow as tf
    n_batch = tf.shape(x)[batch_axis]
    if isinstance(min_num, int) and isinstance(max_num, int) and min_num == max_num:
        num = min_num
    else:
        num = tf.random_uniform(shape=(n_batch,), minval=min_num, maxval=max_num + 1, dtype=tf.int32)
    # https://github.com/tensorflow/tensorflow/issues/9260
    # https://timvieira.github.io/blog/post/2014/08/01/gumbel-max-trick-and-weighted-reservoir-sampling/
    z = -tf.log(-tf.log(tf.random_uniform((n_batch, tf.shape(x)[axis]), 0, 1)))
    _, indices = tf.nn.top_k(z, num if isinstance(num, int) else tf.reduce_max(num))
    # indices should be sorted, and of shape (batch,num), entries (int32) in [0,dim)
    # indices = tf.Print(indices, ["indices", indices, tf.shape(indices)])
    if isinstance(num, int):
        for i in range(num):
            x = _mask(x, batch_axis=batch_axis, axis=axis, pos=indices[:, i], max_amount=max_dims)
    else:
        _, x = tf.while_loop(
            cond=lambda i, _: tf.less(i, tf.reduce_max(num)),
            body=lambda i, x: (
                i + 1,
                tf.where(
                    tf.less(i, num),
                    _mask(x, batch_axis=batch_axis, axis=axis, pos=indices[:, i], max_amount=max_dims),
                    x)),
            loop_vars=(0, x))
    return x


def transform(data, network, clip=False, time_factor=1):
    x = data.placeholder
    import tensorflow as tf
    from TFUtil import dropout
    # summary("features", x)
    if clip:
        x = tf.clip_by_value(x, -3.0, 3.0)
    step = network.global_train_step
    step1 = tf.where(tf.greater_equal(step, 1000), 1, 0)
    step2 = tf.where(tf.greater_equal(step, 2000), 1, 0)
    drop_keep_min = 0.7
    drop_keep = tf.exp(-tf.cast(step, tf.float32) * 0.00001) * (1. - drop_keep_min) + drop_keep_min
    #summary("features_clip", x)
    def get_masked():
        x_masked = x
        x_masked = random_mask(
          x_masked, batch_axis=data.batch_dim_axis, axis=data.time_dim_axis,
          min_num=step1, max_num=tf.maximum(tf.shape(x)[data.time_dim_axis] // 100, tf.maximum(step1, 2)) * (2 + step2 * 2) // 2,
          max_dims=20 // time_factor)
        x_masked = random_mask(
          x_masked, batch_axis=data.batch_dim_axis, axis=data.feature_dim_axis,
          min_num=step1, max_num=2 + step2 * 2,
          max_dims=data.dim // 5)
        #x_masked = dropout(
        #    x_masked, keep_prob=drop_keep, apply_correction_factor=False)
        #summary("features_mask", x_masked)
        return x_masked
    x = network.cond_on_train(get_masked, lambda: x)
    return x


network = {
"source": {"class": "eval", "eval": "self.network.get_config().typed_value('transform')(source(0, as_data=True), network=self.network, clip=True)", "trainable": False},
"model0_lstm0_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": 1, "from": ["source"] , "trainable": False},
"model0_lstm0_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": -1, "from": ["source"] , "trainable": False},
"model0_lstm0_pool": {"class": "pool", "mode": "max", "padding": "same", "pool_size": (3,), "from": ["model0_lstm0_fw", "model0_lstm0_bw"], "trainable": False},

"model0_lstm1_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": 1, "from": ["model0_lstm0_pool"], "dropout": 0.3 , "trainable": False},
"model0_lstm1_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": -1, "from": ["model0_lstm0_pool"], "dropout": 0.3 , "trainable": False},
"model0_lstm1_pool": {"class": "pool", "mode": "max", "padding": "same", "pool_size": (2,), "from": ["model0_lstm1_fw", "model0_lstm1_bw"], "trainable": False},

"model0_lstm2_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": 1, "from": ["model0_lstm1_pool"], "dropout": 0.3 , "trainable": False},
"model0_lstm2_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": -1, "from": ["model0_lstm1_pool"], "dropout": 0.3 , "trainable": False},
"model0_lstm2_pool": {"class": "pool", "mode": "max", "padding": "same", "pool_size": (1,), "from": ["model0_lstm2_fw", "model0_lstm2_bw"], "trainable": False},

"model0_lstm3_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": 1, "from": ["model0_lstm2_pool"], "dropout": 0.3 , "trainable": False},
"model0_lstm3_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": -1, "from": ["model0_lstm2_pool"], "dropout": 0.3 , "trainable": False},
"model0_lstm3_pool": {"class": "pool", "mode": "max", "padding": "same", "pool_size": (1,), "from": ["model0_lstm3_fw", "model0_lstm3_bw"], "trainable": False},

"model0_lstm4_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": 1, "from": ["model0_lstm3_pool"], "dropout": 0.3 , "trainable": False},
"model0_lstm4_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": -1, "from": ["model0_lstm3_pool"], "dropout": 0.3 , "trainable": False},
"model0_lstm4_pool": {"class": "pool", "mode": "max", "padding": "same", "pool_size": (1,), "from": ["model0_lstm4_fw", "model0_lstm4_bw"], "trainable": False},

"model0_lstm5_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": 1, "from": ["model0_lstm4_pool"], "dropout": 0.3 , "trainable": False},
"model0_lstm5_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": -1, "from": ["model0_lstm4_pool"], "dropout": 0.3 , "trainable": False},

"model0_encoder": {"class": "copy", "from": ["model0_lstm5_fw", "model0_lstm5_bw"], "trainable": False},  # dim: EncValueTotalDim
"model0_enc_ctx": {"class": "linear", "activation": None, "with_bias": True, "from": ["model0_encoder"], "n_out": EncKeyTotalDim, "trainable": False},  # preprocessed_attended in Blocks
"model0_inv_fertility": {"class": "linear", "activation": "sigmoid", "with_bias": False, "from": ["model0_encoder"], "n_out": AttNumHeads, "trainable": False},
"model0_enc_value": {"class": "split_dims", "axis": "F", "dims": (AttNumHeads, EncValuePerHeadDim), "from": ["model0_encoder"], "trainable": False},  # (B, enc-T, H, D'/H)

"model0_output": {"class": "rec", "from": [], 'cheating': config.bool("cheating", False), "unit": {
    'output': {'class': 'choice', 'target': target_asr, 'beam_size': beam_size_asr, 'cheating': config.bool("cheating", False), 'from': ["output_prob"], "initial_output": 0, "trainable": False},
    "end": {"class": "compare", "from": ["output"], "value": 0, "trainable": False},
    'target_embed': {'class': 'linear', 'activation': None, "with_bias": False, 'from': ['output'], "n_out": 620, "initial_output": 0, "trainable": False},  # feedback_input
    "weight_feedback": {"class": "linear", "activation": None, "with_bias": False, "from": ["prev:accum_att_weights"], "n_out": EncKeyTotalDim, "trainable": False},
    "s_transformed": {"class": "linear", "activation": None, "with_bias": False, "from": ["s"], "n_out": EncKeyTotalDim, "trainable": False},
    "energy_in": {"class": "combine", "kind": "add", "from": ["base:model0_enc_ctx", "weight_feedback", "s_transformed"], "n_out": EncKeyTotalDim, "trainable": False},
    "energy_tanh": {"class": "activation", "activation": "tanh", "from": ["energy_in"], "trainable": False},
    "energy": {"class": "linear", "activation": None, "with_bias": False, "from": ["energy_tanh"], "n_out": AttNumHeads, "trainable": False},  # (B, enc-T, H)
    "att_weights": {"class": "softmax_over_spatial", "from": ["energy"], "trainable": False},  # (B, enc-T, H)
    "accum_att_weights": {"class": "eval", "from": ["prev:accum_att_weights", "att_weights", "base:model0_inv_fertility"],
        "eval": "source(0) + source(1) * source(2) * 0.5", "out_type": {"dim": AttNumHeads, "shape": (None, AttNumHeads)}, "trainable": False},
    "att0": {"class": "generic_attention", "weights": "att_weights", "base": "base:model0_enc_value", "trainable": False},  # (B, H, V)
    "att": {"class": "merge_dims", "axes": "except_batch", "from": ["att0"], "trainable": False},  # (B, H*V)
    "s": {"class": "rnn_cell", "unit": "LSTMBlock", "from": ["prev:target_embed", "prev:att"], "n_out": 1000, "trainable": False},  # transform
    "readout_in": {"class": "linear", "from": ["s", "prev:target_embed", "att"], "activation": None, "n_out": 1000, "trainable": False},  # merge + post_merge bias
    "readout": {"class": "reduce_out", "mode": "max", "num_pieces": 2, "from": ["readout_in"], "trainable": False},
    "output_prob": {
        "class": "softmax", "from": ["readout"], "dropout": 0.3, "is_output_layer": True,
        "target": target_asr, "loss": "ce", "trainable": False},

    #"output_prob_src_beams": {"class": "choice_get_src_beams", "from": ["output"], "is_output_layer": True, "only_on_search": True},
    #"output_prob_beam_scores": {"class": "choice_get_beam_scores", "from": ["output"], "is_output_layer": True, "only_on_search": True},
}, "target": target_asr, "max_seq_len": "max_len_from('base:model0_encoder')", "trainable": False},

#"posterior": {"class": "get_beamed_layer", "scores": "model0_output/output_prob_beam_scores", "beams": "model0_output/output_prob_src_beams", "debug": False, "beam_size": beam_size_asr, "mode": "best", "beam_idx": config.int("beam_idx", 0), "one_hot": False, "from": ["model0_output/output_prob"], "only_on_search": True},  # "output2/choice_get_beam_scores"

"posterior": {"class": "copy", "from": ["model0_output/output_prob"]},

'max': {'class': 'reduce', 'mode': 'max', 'from': ['posterior'], 'axis': 'F'},
'div2': {'class': 'eval', 'eval': 'tf.divide(source(0), source(1))', 'from': ['posterior', 'max']},
'power': {'class': 'eval', 'eval': 'tf.pow(source(0), %s)' % str(config.float("prob_scale", 1.0)), 'from': ['div2']},
'sum': {'class': 'reduce', 'mode': 'sum', 'from': ['power'], 'axis': 'F'},
'div': {'class': 'eval', 'eval': 'tf.divide(source(0), source(1))', 'from': ['power', 'sum']},

'add_end': {'class': 'addonehot', 'from': ['div'], "vocab_size": 5131},

"model1_source_embed": {"class": "linear", "activation": None, "with_bias": False, "n_out": 512, "from": ["add_end"]},

"model1_lstm0_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": 1, "from": ["model1_source_embed"] },
"model1_lstm0_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": -1, "from": ["model1_source_embed"] },

"model1_lstm1_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": 1, "from": ["model1_lstm0_fw", "model1_lstm0_bw"] },
"model1_lstm1_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": -1, "from": ["model1_lstm0_fw", "model1_lstm0_bw"] },

"model1_lstm2_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": 1, "from": ["model1_lstm1_fw", "model1_lstm1_bw"] },
"model1_lstm2_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": -1, "from": ["model1_lstm1_fw", "model1_lstm1_bw"] },

"model1_lstm3_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": 1, "from": ["model1_lstm2_fw", "model1_lstm2_bw"] },
"model1_lstm3_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": -1, "from": ["model1_lstm2_fw", "model1_lstm2_bw"] },

"model1_lstm4_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": 1, "from": ["model1_lstm3_fw", "model1_lstm3_bw"] },
"model1_lstm4_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": -1, "from": ["model1_lstm3_fw", "model1_lstm3_bw"] },

"model1_lstm5_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": 1, "from": ["model1_lstm4_fw", "model1_lstm4_bw"] },
"model1_lstm5_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": -1, "from": ["model1_lstm4_fw", "model1_lstm4_bw"] },

"model1_encoder": {"class": "copy", "from": ["model1_lstm5_fw", "model1_lstm5_bw"]},  # dim: EncValueTotalDim
"model1_enc_ctx": {"class": "linear", "activation": None, "with_bias": True, "from": ["model1_encoder"], "n_out": EncKeyTotalDim},  # preprocessed_attended in Blocks
"model1_inv_fertility": {"class": "linear", "activation": "sigmoid", "with_bias": False, "from": ["model1_encoder"], "n_out": AttNumHeads},
"model1_enc_value": {"class": "split_dims", "axis": "F", "dims": (AttNumHeads, EncValuePerHeadDim), "from": ["model1_encoder"]},  # (B, enc-T, H, D'/H)

"output": {"class": "rec", "from": [], "unit": {
    'output': {'class': 'choice', 'target': target_st, 'beam_size': beam_size, 'from': ["model1_output_prob"], "initial_output": 0},
    "end": {"class": "compare", "from": ["output"], "value": 0},
    'model1_target_embed': {'class': 'linear', 'activation': None, "with_bias": False, 'from': ['output'], "n_out": 620, "initial_output": 0},  # feedback_input
    "model1_weight_feedback": {"class": "linear", "activation": None, "with_bias": False, "from": ["prev:model1_accum_att_weights"], "n_out": EncKeyTotalDim},
    "model1_s_transformed": {"class": "linear", "activation": None, "with_bias": False, "from": ["model1_s"], "n_out": EncKeyTotalDim},
    "model1_energy_in": {"class": "combine", "kind": "add", "from": ["base:model1_enc_ctx", "model1_weight_feedback", "model1_s_transformed"], "n_out": EncKeyTotalDim},
    "model1_energy_tanh": {"class": "activation", "activation": "tanh", "from": ["model1_energy_in"]},
    "model1_energy": {"class": "linear", "activation": None, "with_bias": False, "from": ["model1_energy_tanh"], "n_out": AttNumHeads},  # (B, enc-T, H)
    "model1_att_weights": {"class": "softmax_over_spatial", "from": ["model1_energy"]},  # (B, enc-T, H)
    "model1_accum_att_weights": {"class": "eval", "from": ["prev:model1_accum_att_weights", "model1_att_weights", "base:model1_inv_fertility"],
        "eval": "source(0) + source(1) * source(2) * 0.5", "out_type": {"dim": AttNumHeads, "shape": (None, AttNumHeads)}},
    "model1_att0": {"class": "generic_attention", "weights": "model1_att_weights", "base": "base:model1_enc_value"},  # (B, H, V)
    "model1_att": {"class": "merge_dims", "axes": "except_batch", "from": ["model1_att0"]},  # (B, H*V)
    "model1_s": {"class": "rnn_cell", "unit": "LSTMBlock", "from": ["prev:model1_target_embed", "prev:model1_att"], "n_out": 1000},  # transform
    "model1_readout_in": {"class": "linear", "from": ["model1_s", "prev:model1_target_embed", "model1_att"], "activation": None, "n_out": 1000},  # merge + post_merge bias
    "model1_readout": {"class": "reduce_out", "mode": "max", "num_pieces": 2, "from": ["model1_readout_in"]},
    "model1_output_prob": {
        "class": "softmax", "from": ["model1_readout"], "dropout": 0.3,
        "target": target_st, "loss": "ce", "loss_opts": {"label_smoothing": 0.1}}
}, "target": target_st, "max_seq_len": "max_len_from('base:model0_encoder') * 3"},

"decision": {
    "class": "decide", "from": ["output"], "loss": "edit_distance", "target": target_st,
    "loss_opts": {
        }
    }
}

# models for ensembling
ens_model_file_0 = "/work/bahar/st/iwslt2020/training/asr--20200203/data-train/test-best18-specaug-5k-2M-dev/network.150"
ens_model_prefix_0 = "model0_"
ens_model_file_1 = "/work/bahar/st/iwslt2020/training/mt--20200203/data-train/mt-rnn6-24m-en-bpe5k/network.103"
ens_model_prefix_1 = "model1_"

# model 0 is loaded as usual
preload_from_files = {}
if task == "search":
  network["model0_output"]["unit"].update({
        "output_prob_src_beams": {"class": "choice_get_src_beams", "from": ["output"], "is_output_layer": True, "only_on_search": True},
        "output_prob_beam_scores": {"class": "choice_get_beam_scores", "from": ["output"], "is_output_layer": True, "only_on_search": True},
      })
  network.update({
        "posterior": {"class": "get_beamed_layer", "scores": "model0_output/output_prob_beam_scores", "beams": "model0_output/output_prob_src_beams", "debug": False, "beam_size": beam_size_asr, "mode": "best", "beam_idx": config.int("beam_idx", 0), "one_hot": False, "from": ["model0_output/output_prob"], "only_on_search": True},  # "output2/choice_get_beam_scores"
      })
else:
  preload_from_files = {
    "model0" : {"filename": ens_model_file_0, "prefix": ens_model_prefix_0, "init_for_train": True},
    "model1" : {"filename": ens_model_file_1, "prefix": ens_model_prefix_1, "init_for_train": True},
  }

search_output_layer = "decision"
debug_print_layer_output_template = True
debug_add_check_numerics_on_output = False
debug_add_check_numerics_ops = False

# trainer
batching = "random"
log_batch_size = True
accum_grad_multiple_step = 3
batch_size = 5000
max_seqs = 200
max_seq_length = {"target_text": 75, "data": 6000}
#chunking = ""  # no chunking
truncation = -1

tf_log_memory_usage = True
num_epochs = 150
model = "net-model/network"
cleanup_old_models = False
gradient_clip = 0
#gradient_clip_global_norm = 1.0
adam = True
optimizer_epsilon = 1e-8
#debug_add_check_numerics_ops = True
#debug_add_check_numerics_on_output = True
stop_on_nonfinite_train_score = False
tf_log_memory_usage = True
gradient_noise = 0.0
learning_rate = 0.00008
learning_rates = list(numpy.linspace(0.00003, learning_rate, num=5))  # warmup
min_learning_rate = learning_rate / 50.
learning_rate_control = "newbob_multi_epoch"
#learning_rate_control_error_measure = "dev_score_output"
learning_rate_control_relative_error_relative_lr = True
learning_rate_control_min_num_epochs_per_new_lr = 3
use_learning_rate_control_always = True
newbob_multi_num_epochs = 6
newbob_multi_update_interval = 1
newbob_learning_rate_decay = 0.9
learning_rate_file = "newbob.data"

# log
log_verbosity = 5
