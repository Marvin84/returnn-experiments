Pre-trained models can be downloaded from [here](http://www-i6.informatik.rwth-aachen.de/~irie/models/librispeech/2019-lm-transformers)

If you make use of these models, please cite this paper:
```
@article{irie2019language,
  title={Language Modeling with Deep Transformers},
  author={Irie, Kazuki and Zeyer, Albert and Schl{\"u}ter, Ralf and Ney, Hermann},
  journal={arXiv preprint arXiv:1905.04226, submitted to INTERSPEECH 2019},
  year={2019}
}
```
