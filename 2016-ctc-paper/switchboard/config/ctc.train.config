include setup.base.config
include network.config

# ---------------------------------------------------------------------------
[*]
job-name                    = train
use-cuda					= false # This is for Sprint only.
seed						= 1  # Default.
allophone-file				= dependencies/allophones
log-channel.file			= log/sprint.train.log

# ---------------------------------------------------------------------------
[*]
action                      = supervised-segmentwise-training
trainer                     = feed-forward-trainer
training-criterion          = ctc
estimator                   = dry-run

# For RNNs, we need full unshuffled segments.
buffer-type					= utterance
buffer-size					= 200000 # the maximum utterance length
shuffle						= false # buffer must not be shuffled


# ---------------------------------------------------------------------------
# Baum-Welch alignment

# For some older configs, see e.g. here:
# file:///u/zeyer/setups/quaero-en/training/quaero-train11/50h/ann/6x2048-ctc-pretained/config/training.config

[*]

input-in-log-space			= true		# We get log-probs from Python.

state-posterior-scale		= 1

#posterior-nbest-limit		= 5
#posterior-scale				= 1

use-search-aligner			= true

#debug-dumps = true

[*.ctc-aligner]
mode						= baum-welch
search.log-variability		= false
log-iterations				= true

min-acoustic-pruning            = 400
#min-acoustic-pruning          = 50000
#max-acoustic-pruning            = 3200
max-acoustic-pruning          = 1000000
min-average-number-of-states        = 12
n-best-pruning			= 500	# this is the important pruning. max number of states per time-frame

increase-pruning-until-no-score-difference = true
#increase-pruning-until-no-score-difference = false
acoustic-pruning-increment-factor = 2 # use '2' to auto increase

collect-garbage-time		= 100	# just for performance

statistics.channel          = log-channel



[*.hmm]
states-per-phone                = 3
state-repetitions               = 1
across-word-model               = yes
early-recombination             = no

[*.tdp]
scale				= 1.0
*.loop				= 3.0
*.forward			= 0.0
*.skip				= 30.0
*.exit				= 0.0
entry-m1.loop			= infinity
entry-m2.loop			= infinity
silence.loop                    = 0.0
silence.forward                 = 3.0
silence.skip                    = infinity
silence.exit                    = 20.0
#nonword-0.exit      = 0


[*]

prior-learning-rate			= 0
prior-file                  = dependencies/prior-f32.xml
priori-scale                = 0.6


# ---------------------------------------------------------------------------
# Python related. Similar to seq-train.train.config.

# Neural network, PythonLayer (which returns log-probs)
[*.neural-network]
links                       = 0->python-layer:0

[*.python-layer]
layer-type                  = python
dimension-input             = 40	# input is ignored
dimension-output            = $(number-of-classes)
