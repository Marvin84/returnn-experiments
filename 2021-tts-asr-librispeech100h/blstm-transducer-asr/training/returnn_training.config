#!rnn.py

batch_size = 10000
batching = 'random'
cache_size = '0'
chunking = ({'classes': 128, 'data': 256}, {'classes': 64, 'data': 128})
cleanup_old_models = False
dev = { 'class': 'ExternSprintDataset',
  'partitionEpoch': 1,
  'reduce_target_factor': 2,
  'sprintConfigStr': '--config=sprint.dev.config --*.LOGFILE=nn-trainer.dev.log --*.TASK=1 --*.corpus.segment-order-shuffle=true '
                     '--*.segment-order-sort-by-time-length=true --*.segment-order-sort-by-time-length-chunk-size=128',
  'sprintTrainerExecPath': '/u/zhou/rasr-dev/arch/linux-x86_64-standard-label_sync_decoding/nn-trainer.linux-x86_64-standard-label_sync_decoding'}
device = 'gpu'
gradient_clip = 5
gradient_noise = 0.1
learning_rate = 0.001
learning_rate_control = 'newbob_multi_epoch'
learning_rate_control_min_num_epochs_per_new_lr = 3
learning_rate_control_relative_error_relative_lr = True
learning_rate_file = 'learning_rates'
learning_rates = [0.001] * 30
log = ['./crnn.log']
log_verbosity = 3
max_seqs = 128
min_learning_rate = 2e-05
model = 'output/models/epoch'
multiprocessing = True
nadam = True
network = { 'bwd_lstm_1': {'L2': 0.01, 'class': 'rec', 'direction': -1, 'dropout': 0.1, 'from': 'source', 'n_out': 512, 'unit': 'nativelstm2'},
  'bwd_lstm_2': { 'L2': 0.01,
                  'class': 'rec',
                  'direction': -1,
                  'dropout': 0.1,
                  'from': ['fwd_lstm_1', 'bwd_lstm_1'],
                  'n_out': 512,
                  'unit': 'nativelstm2'},
  'bwd_lstm_3': { 'L2': 0.01,
                  'class': 'rec',
                  'direction': -1,
                  'dropout': 0.1,
                  'from': ['fwd_lstm_2', 'bwd_lstm_2'],
                  'n_out': 512,
                  'unit': 'nativelstm2'},
  'bwd_lstm_4': {'L2': 0.01, 'class': 'rec', 'direction': -1, 'dropout': 0.1, 'from': 'max_pool_3', 'n_out': 512, 'unit': 'nativelstm2'},
  'bwd_lstm_5': { 'L2': 0.01,
                  'class': 'rec',
                  'direction': -1,
                  'dropout': 0.1,
                  'from': ['fwd_lstm_4', 'bwd_lstm_4'],
                  'n_out': 512,
                  'unit': 'nativelstm2'},
  'bwd_lstm_6': { 'L2': 0.01,
                  'class': 'rec',
                  'direction': -1,
                  'dropout': 0.1,
                  'from': ['fwd_lstm_5', 'bwd_lstm_5'],
                  'n_out': 512,
                  'unit': 'nativelstm2'},
  'enc_output': {'class': 'softmax', 'from': 'encoder', 'loss': 'ce', 'loss_opts': {'focal_loss_factor': 1.0}},
  'encoder': {'class': 'reinterpret_data', 'from': ['fwd_lstm_6', 'bwd_lstm_6'], 'size_base': 'data:classes'},
  'fwd_lstm_1': {'L2': 0.01, 'class': 'rec', 'direction': 1, 'dropout': 0.1, 'from': 'source', 'n_out': 512, 'unit': 'nativelstm2'},
  'fwd_lstm_2': { 'L2': 0.01,
                  'class': 'rec',
                  'direction': 1,
                  'dropout': 0.1,
                  'from': ['fwd_lstm_1', 'bwd_lstm_1'],
                  'n_out': 512,
                  'unit': 'nativelstm2'},
  'fwd_lstm_3': { 'L2': 0.01,
                  'class': 'rec',
                  'direction': 1,
                  'dropout': 0.1,
                  'from': ['fwd_lstm_2', 'bwd_lstm_2'],
                  'n_out': 512,
                  'unit': 'nativelstm2'},
  'fwd_lstm_4': {'L2': 0.01, 'class': 'rec', 'direction': 1, 'dropout': 0.1, 'from': 'max_pool_3', 'n_out': 512, 'unit': 'nativelstm2'},
  'fwd_lstm_5': { 'L2': 0.01,
                  'class': 'rec',
                  'direction': 1,
                  'dropout': 0.1,
                  'from': ['fwd_lstm_4', 'bwd_lstm_4'],
                  'n_out': 512,
                  'unit': 'nativelstm2'},
  'fwd_lstm_6': { 'L2': 0.01,
                  'class': 'rec',
                  'direction': 1,
                  'dropout': 0.1,
                  'from': ['fwd_lstm_5', 'bwd_lstm_5'],
                  'n_out': 512,
                  'unit': 'nativelstm2'},
  'mask_flag': {'class': 'compare', 'from': ['data:classes'], 'kind': 'not_equal', 'value': 0},
  'mask_label': {'class': 'masked_computation', 'from': ['data:classes'], 'mask': 'mask_flag', 'unit': {'class': 'copy'}},
  'max_pool_3': {'class': 'pool', 'from': ['fwd_lstm_3', 'bwd_lstm_3'], 'mode': 'max', 'padding': 'same', 'pool_size': (2,), 'trainable': False},
  'output': { 'cheating': False,
              'class': 'rec',
              'from': 'encoder',
              'target': 'classes',
              'unit': { 'ce_loss': {'class': 'loss', 'from': 'output', 'loss_': 'ce'},
                        'embedding': {'L2': 0.01, 'activation': None, 'class': 'linear', 'from': 'base:mask_label', 'n_out': 128, 'with_bias': False},
                        'joint_encoding': {'class': 'combine', 'from': ['data:source', 'unmask_lm_reinterpret'], 'kind': 'add'},
                        'lm_masked': { 'class': 'masked_computation',
                                       'from': [],
                                       'mask': None,
                                       'masked_from': 'mask_embedding',
                                       'unit': { 'class': 'subnetwork',
                                                 'from': 'data',
                                                 'subnetwork': { 'lm_lstm_1': { 'L2': 0.01,
                                                                                'class': 'rec',
                                                                                'direction': 1,
                                                                                'dropout': 0.1,
                                                                                'from': 'data',
                                                                                'n_out': 1024,
                                                                                'unit': 'nativelstm2'},
                                                                 'lm_lstm_2': { 'L2': 0.01,
                                                                                'class': 'rec',
                                                                                'direction': 1,
                                                                                'dropout': 0.1,
                                                                                'from': 'lm_lstm_1',
                                                                                'n_out': 1024,
                                                                                'unit': 'nativelstm2'},
                                                                 'output': {'class': 'copy', 'from': 'lm_lstm_2'}}}},
                        'mask_embedding': {'axes': 'T', 'class': 'pad', 'from': 'embedding', 'mode': 'constant', 'padding': (1, 0), 'value': 0},
                        'mask_flag': {'amount': 1, 'axis': 'T', 'class': 'shift_axis', 'from': 'base:mask_flag', 'pad': True},
                        'output': {'class': 'softmax', 'from': 'joint_encoding', 'loss': 'ce', 'loss_opts': {'label_smoothing': 0.2}},
                        'segmental_loss': { 'class': 'eval',
                                            'eval': "self.network.get_config().typed_value('segmental_loss')(source(0), source(1))",
                                            'from': ['ce_loss', 'base:mask_flag'],
                                            'loss': 'as_is',
                                            'loss_opts': {'scale': 5.0}},
                        'unmask_lm': {'class': 'unmask', 'from': 'lm_masked', 'mask': 'mask_flag', 'skip_initial': True},
                        'unmask_lm_reinterpret': {'class': 'reinterpret_data', 'from': 'unmask_lm', 'size_base': 'data:classes'}}},
  'source': {'class': 'eval', 'eval': "self.network.get_config().typed_value('transform')(source(0, as_data=True), network=self.network)"}}
newbob_learning_rate_decay = 0.9
newbob_multi_num_epochs = 3
newbob_multi_update_interval = 1
num_epochs = 300
num_inputs = 50
num_outputs = 139
optimizer_epsilon = 1e-08
pretrain = 'default'
save_interval = 1
start_batch = 'auto'
start_epoch = 'auto'
target = 'classes'
task = 'train'
train = { 'class': 'ExternSprintDataset',
  'partitionEpoch': 3,
  'reduce_target_factor': 2,
  'sprintConfigStr': '--config=sprint.train.config --*.LOGFILE=nn-trainer.train.log --*.TASK=1 --*.corpus.segment-order-shuffle=true '
                     '--*.segment-order-sort-by-time-length=true --*.segment-order-sort-by-time-length-chunk-size=384',
  'sprintTrainerExecPath': '/u/zhou/rasr-dev/arch/linux-x86_64-standard-label_sync_decoding/nn-trainer.linux-x86_64-standard-label_sync_decoding'}
truncation = -1
update_on_device = True
use_tensorflow = True
window = 1
config = {}

locals().update(**config)


# idx start at 0
def pretrain_construction_algo(idx, net_dict, encoder_lstm='fwd_lstm', pool_name='max_pool', encoder_output='enc_output'):
  orig_num_lstm_layers = 0
  down_sample_idx = 0
  while "%s_%i" %(encoder_lstm, orig_num_lstm_layers+1) in net_dict:
    orig_num_lstm_layers += 1
    if "%s_%i" %(pool_name, orig_num_lstm_layers) in net_dict:
      down_sample_idx = orig_num_lstm_layers
  assert orig_num_lstm_layers > 0, 'no encoder network ?'

  # finish
  num_lstm_layers = idx + 1
  if num_lstm_layers > orig_num_lstm_layers:
    return None

  # full encoder pretraining
  if encoder_output in net_dict:
    del net_dict[encoder_output]
  net_dict[encoder_output] = {'class': 'softmax', 'from': 'encoder', 'loss': 'ce'}

  # remove decoder and loss 
  del net_dict['output']
  layers = list(net_dict.keys())
  for layer in layers:
    for name_pattern in ['mask', '_loss', 'shift_']:
      if name_pattern in layer:
        del net_dict[layer]
        break 
  return net_dict

# how many (sub)epochs for one pretrain step #
pretrain = {'repetitions': [5, 5, 5, 5, 5, 5, 1], 'construction_algo': pretrain_construction_algo}

  

def segmental_loss(loss, mask):
  import tensorflow as tf
  # (T, B)
  blanks = tf.where(mask, tf.zeros_like(loss, dtype=tf.float32), tf.ones_like(loss, dtype=tf.float32))
  count = tf.reduce_sum(blanks, axis=0, keepdims=True)
  seg_loss = loss / count # broadcasting
  final_loss = tf.where(mask, loss, seg_loss)
  return final_loss

# for debug only
def summary(name, x):
  """
  :param str name:
  :param tf.Tensor x: (batch,time,feature)
  """
  import tensorflow as tf
  # tf.summary.image wants [batch_size, height,  width, channels],
  # we have (batch, time, feature).
  img = tf.expand_dims(x, axis=3)  # (batch,time,feature,1)
  img = tf.transpose(img, [0, 2, 1, 3])  # (batch,feature,time,1)
  tf.summary.image(name, img, max_outputs=10)
  tf.summary.scalar("%s_max_abs" % name, tf.reduce_max(tf.abs(x)))
  mean = tf.reduce_mean(x)
  tf.summary.scalar("%s_mean" % name, mean)
  stddev = tf.sqrt(tf.reduce_mean(tf.square(x - mean)))
  tf.summary.scalar("%s_stddev" % name, stddev)
  tf.summary.histogram("%s_hist" % name, tf.reduce_max(tf.abs(x), axis=2))


def _mask(x, batch_axis, axis, pos, max_amount):
  """
  :param tf.Tensor x: (batch,time,feature)
  :param int batch_axis:
  :param int axis:
  :param tf.Tensor pos: (batch,)
  :param int|tf.Tensor max_amount: inclusive
  """
  import tensorflow as tf
  ndim = x.get_shape().ndims
  n_batch = tf.shape(x)[batch_axis]
  dim = tf.shape(x)[axis]
  amount = tf.random_uniform(shape=(n_batch,), minval=1, maxval=max_amount + 1, dtype=tf.int32)
  pos2 = tf.minimum(pos + amount, dim)
  idxs = tf.expand_dims(tf.range(0, dim), 0)  # (1,dim)
  pos_bc = tf.expand_dims(pos, 1)  # (batch,1)
  pos2_bc = tf.expand_dims(pos2, 1)  # (batch,1)
  cond = tf.logical_and(tf.greater_equal(idxs, pos_bc), tf.less(idxs, pos2_bc))  # (batch,dim)
  if batch_axis > axis:
    cond = tf.transpose(cond)  # (dim,batch)
  cond = tf.reshape(cond, [tf.shape(x)[i] if i in (batch_axis, axis) else 1 for i in range(ndim)])
  from TFUtil import where_bc
  x = where_bc(cond, 0.0, x)
  return x


def random_mask(x, batch_axis, axis, min_num, max_num, max_dims):
  """
  :param tf.Tensor x: (batch,time,feature)
  :param int batch_axis:
  :param int axis:
  :param int|tf.Tensor min_num:
  :param int|tf.Tensor max_num: inclusive
  :param int|tf.Tensor max_dims: inclusive
  """
  import tensorflow as tf
  n_batch = tf.shape(x)[batch_axis]
  if isinstance(min_num, int) and isinstance(max_num, int) and min_num == max_num:
    num = min_num
  else:
    num = tf.random_uniform(shape=(n_batch,), minval=min_num, maxval=max_num + 1, dtype=tf.int32)
  # https://github.com/tensorflow/tensorflow/issues/9260
  # https://timvieira.github.io/blog/post/2014/08/01/gumbel-max-trick-and-weighted-reservoir-sampling/
  z = -tf.log(-tf.log(tf.random_uniform((n_batch, tf.shape(x)[axis]), 0, 1)))
  _, indices = tf.nn.top_k(z, num if isinstance(num, int) else tf.reduce_max(num))
  # indices should be sorted, and of shape (batch,num), entries (int32) in [0,dim)
  # indices = tf.Print(indices, ["indices", indices, tf.shape(indices)])
  if isinstance(num, int):
    for i in range(num):
      x = _mask(x, batch_axis=batch_axis, axis=axis, pos=indices[:, i], max_amount=max_dims)
  else:
    _, x = tf.while_loop( cond=lambda i, _: tf.less(i, tf.reduce_max(num)),
                          body=lambda i, x: ( i + 1,
                                              tf.where( tf.less(i, num),
                                                        _mask(x, batch_axis=batch_axis, axis=axis, pos=indices[:, i], max_amount=max_dims),
                                                        x ) 
                                            ),
                          loop_vars=(0, x)
                        )
  return x


def transform(data, network):
  # to be adjusted (20-50%)
  max_time_num = 1
  max_time = 15

  max_feature_num = 5
  max_feature = 5

  # halved before this step
  conservatvie_step = 2000

  x = data.placeholder
  import tensorflow as tf
  # summary("features", x)
  step = network.global_train_step
  increase_flag = tf.where(tf.greater_equal(step, conservatvie_step), 0, 1)

  def get_masked():
    x_masked = x
    x_masked = random_mask( x_masked, batch_axis=data.batch_dim_axis, axis=data.time_dim_axis,
                            min_num=0, max_num=tf.maximum(tf.shape(x)[data.time_dim_axis]//int(1/0.70*max_time), max_time_num) // (1+increase_flag),
                            max_dims=max_time
                          )
    x_masked = random_mask( x_masked, batch_axis=data.batch_dim_axis, axis=data.feature_dim_axis,
                            min_num=0, max_num=max_feature_num // (1+increase_flag),
                            max_dims=max_feature
                          )
    #summary("features_mask", x_masked)
    return x_masked
  x = network.cond_on_train(get_masked, lambda: x)
  return x


