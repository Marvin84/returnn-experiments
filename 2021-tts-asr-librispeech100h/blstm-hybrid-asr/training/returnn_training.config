#!rnn.py



def _mask(x, axis, pos, max_amount):
    """
    :param tf.Tensor x: (batch,time,feature)
    :param int axis:
    :param tf.Tensor pos: (batch,)
    :param int max_amount: inclusive
    """
    import tensorflow as tf
    ndim = x.get_shape().ndims
    n_batch = tf.shape(x)[0]
    dim = tf.shape(x)[axis]
    amount = tf.random_uniform(shape=(n_batch,), minval=1, maxval=max_amount + 1, dtype=tf.int32)
    pos2 = tf.minimum(pos + amount, dim)
    idxs = tf.expand_dims(tf.range(0, dim), 0)  # (1,dim)
    pos_bc = tf.expand_dims(pos, 1)  # (batch,1)
    pos2_bc = tf.expand_dims(pos2, 1)  # (batch,1)
    cond = tf.logical_and(tf.greater_equal(idxs, pos_bc), tf.less(idxs, pos2_bc))  # (batch,dim)
    cond = tf.reshape(cond, [tf.shape(x)[i] if i in (0, axis) else 1 for i in range(ndim)])
    from TFUtil import where_bc
    x = where_bc(cond, 0.0, x)
    return x

def random_mask(x, axis, min_num, max_num, max_dims):
    """
    :param tf.Tensor x: (batch,time,feature)
    :param int axis:
    :param int|tf.Tensor min_num:
    :param int|tf.Tensor max_num: inclusive
    :param int max_dims: inclusive
    """
    import tensorflow as tf
    n_batch = tf.shape(x)[0]
    num = tf.random_uniform(shape=(n_batch,), minval=min_num, maxval=max_num + 1, dtype=tf.int32)
    # https://github.com/tensorflow/tensorflow/issues/9260
    # https://timvieira.github.io/blog/post/2014/08/01/gumbel-max-trick-and-weighted-reservoir-sampling/
    z = -tf.log(-tf.log(tf.random_uniform((n_batch, tf.shape(x)[axis]), 0, 1)))
    _, indices = tf.nn.top_k(z, tf.reduce_max(num))
    # indices should be sorted, and of shape (batch,num), entries (int32) in [0,dim)
    # indices = tf.Print(indices, ["indices", indices, tf.shape(indices)])
    _, x = tf.while_loop(
        cond=lambda i, _: tf.less(i, tf.reduce_max(num)),
        body=lambda i, x: (
            i + 1,
            tf.where(
                tf.less(i, num),
                _mask(x, axis=axis, pos=indices[:, i], max_amount=max_dims),
                x)),
        loop_vars=(0, x))
    return x

def summary(name, x):
    """
    :param str name:
    :param tf.Tensor x: (batch,time,feature)
    """
    import tensorflow as tf
    # tf.summary.image wants [batch_size, height,  width, channels],
    # we have (batch, time, feature).
    img = tf.expand_dims(x, axis=3)  # (batch,time,feature,1)
    img = tf.transpose(img, [0, 2, 1, 3])  # (batch,feature,time,1)
    tf.summary.image(name, img, max_outputs=10)
    tf.summary.scalar("%s_max_abs" % name, tf.reduce_max(tf.abs(x)))
    mean = tf.reduce_mean(x)
    tf.summary.scalar("%s_mean" % name, mean)
    stddev = tf.sqrt(tf.reduce_mean(tf.square(x - mean)))
    tf.summary.scalar("%s_stddev" % name, stddev)
    tf.summary.histogram("%s_hist" % name, tf.reduce_max(tf.abs(x), axis=2))

def transform(x, network):
    import tensorflow as tf
    # summary("features", x)
    x = tf.clip_by_value(x, -3.0, 3.0)
    #summary("features_clip", x)

    def get_masked():
        x_masked = x
        x_masked = random_mask(x_masked, axis=1, min_num=1, max_num=tf.maximum(tf.shape(x)[1] // 100, 1), max_dims=20)
        x_masked = random_mask(x_masked, axis=2, min_num=1, max_num=2, max_dims=num_inputs // 5)
        #summary("features_mask", x_masked)
        return x_masked
    x = network.cond_on_train(get_masked, lambda: x)
    return x

EpochSplit = 40
batch_size = 5000
batching = 'random'
chunking = '50:25'
cleanup_old_models = False
debug_print_layer_output_template = True
dev = { 'class': 'ExternSprintDataset',
  'partitionEpoch': 1,
  'sprintConfigStr': '--config=sprint.dev.config --*.LOGFILE=nn-trainer.dev.log --*.TASK=1',
  'sprintTrainerExecPath': '/u/rossenbach/bin/sprint/arch/linux-x86_64-standard/nn-trainer.linux-x86_64-standard'}
device = 'gpu'
gradient_clip = 0
gradient_noise = 0.3
learning_rate = 0.00025
learning_rate_control = 'newbob_multi_epoch'
learning_rate_control_relative_error_relative_lr = True
learning_rate_file = 'newbob.data'
learning_rates = [ 2.5e-05,
  4.9999999999999996e-05,
  7.5e-05,
  9.999999999999999e-05,
  0.000125,
  0.00015000000000000001,
  0.000175,
  0.00019999999999999998,
  0.000225,
  0.00025]
log = ['./crnn.log']
log_batch_size = True
log_verbosity = 3
max_seqs = 200
model = '/u/rossenbach/experiments/librispeech_tts/work/crnn/sprint_training/CRNNSprintTrainingJob.qXdx10O2xH3c/output/models/epoch'
multiprocessing = True
nadam = True
newbob_learning_rate_decay = 0.7071067811865475
newbob_multi_num_epochs = 40
newbob_multi_update_interval = 1
num_epochs = 250
num_inputs = 50
num_outputs = {'classes': [12001, 1], 'data': [50, 2]}
optimizer_epsiloon = 1e-08
save_interval = 10
start_batch = 'auto'
start_epoch = 'auto'
target = 'classes'
task = 'train'
tf_log_memory_usage = True
train = { 'class': 'ExternSprintDataset',
  'partitionEpoch': 40,
  'sprintConfigStr': '--config=sprint.train.config --*.LOGFILE=nn-trainer.train.log --*.TASK=1',
  'sprintTrainerExecPath': '/u/rossenbach/bin/sprint/arch/linux-x86_64-standard/nn-trainer.linux-x86_64-standard'}
trauncation = -1
update_on_device = True
use_tensorflow = True
window = 1
network = { 'blstm_bwd_1': {'L2': 0.0, 'class': 'rec', 'direction': -1, 'dropout': 0.0, 'from': ['source'], 'n_out': 1024, 'unit': 'nativelstm2'},
  'blstm_bwd_2': { 'L2': 0.0,
                   'class': 'rec',
                   'direction': -1,
                   'dropout': 0.0,
                   'from': ['blstm_fwd_1', 'blstm_bwd_1'],
                   'n_out': 1024,
                   'unit': 'nativelstm2'},
  'blstm_bwd_3': { 'L2': 0.0,
                   'class': 'rec',
                   'direction': -1,
                   'dropout': 0.0,
                   'from': ['blstm_fwd_2', 'blstm_bwd_2'],
                   'n_out': 1024,
                   'unit': 'nativelstm2'},
  'blstm_bwd_4': { 'L2': 0.0,
                   'class': 'rec',
                   'direction': -1,
                   'dropout': 0.0,
                   'from': ['blstm_fwd_3', 'blstm_bwd_3'],
                   'n_out': 1024,
                   'unit': 'nativelstm2'},
  'blstm_bwd_5': { 'L2': 0.0,
                   'class': 'rec',
                   'direction': -1,
                   'dropout': 0.0,
                   'from': ['blstm_fwd_4', 'blstm_bwd_4'],
                   'n_out': 1024,
                   'unit': 'nativelstm2'},
  'blstm_bwd_6': { 'L2': 0.0,
                   'class': 'rec',
                   'direction': -1,
                   'dropout': 0.0,
                   'from': ['blstm_fwd_5', 'blstm_bwd_5'],
                   'n_out': 1024,
                   'unit': 'nativelstm2'},
  'blstm_bwd_7': { 'L2': 0.0,
                   'class': 'rec',
                   'direction': -1,
                   'dropout': 0.0,
                   'from': ['blstm_fwd_6', 'blstm_bwd_6'],
                   'n_out': 1024,
                   'unit': 'nativelstm2'},
  'blstm_bwd_8': { 'L2': 0.0,
                   'class': 'rec',
                   'direction': -1,
                   'dropout': 0.0,
                   'from': ['blstm_fwd_7', 'blstm_bwd_7'],
                   'n_out': 1024,
                   'unit': 'nativelstm2'},
  'blstm_fwd_1': {'L2': 0.0, 'class': 'rec', 'direction': 1, 'dropout': 0.0, 'from': ['source'], 'n_out': 1024, 'unit': 'nativelstm2'},
  'blstm_fwd_2': { 'L2': 0.0,
                   'class': 'rec',
                   'direction': 1,
                   'dropout': 0.0,
                   'from': ['blstm_fwd_1', 'blstm_bwd_1'],
                   'n_out': 1024,
                   'unit': 'nativelstm2'},
  'blstm_fwd_3': { 'L2': 0.0,
                   'class': 'rec',
                   'direction': 1,
                   'dropout': 0.0,
                   'from': ['blstm_fwd_2', 'blstm_bwd_2'],
                   'n_out': 1024,
                   'unit': 'nativelstm2'},
  'blstm_fwd_4': { 'L2': 0.0,
                   'class': 'rec',
                   'direction': 1,
                   'dropout': 0.0,
                   'from': ['blstm_fwd_3', 'blstm_bwd_3'],
                   'n_out': 1024,
                   'unit': 'nativelstm2'},
  'blstm_fwd_5': { 'L2': 0.0,
                   'class': 'rec',
                   'direction': 1,
                   'dropout': 0.0,
                   'from': ['blstm_fwd_4', 'blstm_bwd_4'],
                   'n_out': 1024,
                   'unit': 'nativelstm2'},
  'blstm_fwd_6': { 'L2': 0.0,
                   'class': 'rec',
                   'direction': 1,
                   'dropout': 0.0,
                   'from': ['blstm_fwd_5', 'blstm_bwd_5'],
                   'n_out': 1024,
                   'unit': 'nativelstm2'},
  'blstm_fwd_7': { 'L2': 0.0,
                   'class': 'rec',
                   'direction': 1,
                   'dropout': 0.0,
                   'from': ['blstm_fwd_6', 'blstm_bwd_6'],
                   'n_out': 1024,
                   'unit': 'nativelstm2'},
  'blstm_fwd_8': { 'L2': 0.0,
                   'class': 'rec',
                   'direction': 1,
                   'dropout': 0.0,
                   'from': ['blstm_fwd_7', 'blstm_bwd_7'],
                   'n_out': 1024,
                   'unit': 'nativelstm2'},
  'output': {'class': 'softmax', 'from': ['blstm_fwd_8', 'blstm_bwd_8'], 'loss': 'ce', 'loss_opts': {'focal_loss_factor': 2.0}},
  'source': {'class': 'eval', 'eval': "self.network.get_config().typed_value('transform')(source(0), network=self.network)"}}
config = {}

locals().update(**config)


